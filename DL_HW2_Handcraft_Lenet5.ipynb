{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPAUWD3Zj0lUTN9Q+7KahBJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/podo47/DL_HW2_Handcraft_LeNet5-Computational_Graph/blob/main/DL_HW2_Handcraft_Lenet5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK2yVHbvCxQ8"
      },
      "source": [
        "# LeNet-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1y4w23pCuO4"
      },
      "source": [
        "## Mount to drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFVWU4lUzzx9",
        "outputId": "71078879-61e5-4b4a-99dc-0a74270db433"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWymwZ1EDC3O"
      },
      "source": [
        "## Part 1 : Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DloSn-46LTli"
      },
      "source": [
        "### Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQGlA59MDECa"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/images/train.txt', sep=\" \",header=None)\n",
        "train_dir = np.array(train[0])\n",
        "train_y = np.array(train[1])\n",
        "\n",
        "valid = pd.read_csv('/content/drive/MyDrive/images/val.txt', sep=\" \",header=None)\n",
        "valid_dir = np.array(valid[0])\n",
        "valid_y = np.array(valid[1])\n",
        "\n",
        "test = pd.read_csv('/content/drive/MyDrive/images/test.txt', sep=\" \",header=None)\n",
        "test_dir = np.array(test[0])\n",
        "test_y = np.array(test[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6BAhqR42q1U"
      },
      "source": [
        "### Read images to array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqlepSy2_-K9"
      },
      "outputs": [],
      "source": [
        "def read_image(imgname):\n",
        "    img = cv2.imread('/content/drive/MyDrive/'+imgname)\n",
        "    img = cv2.resize(img, (28, 28))\n",
        "    return img\n",
        "\n",
        "def read_images_to_array(data_dir):\n",
        "    pool = ThreadPool(processes=2) # 指定使用 2 個進程\n",
        "    X = pool.map(read_image, data_dir)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    X = np.array(X)\n",
        "    return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_1nH6Gm2wcZ"
      },
      "source": [
        "#### Save data (can skip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDctbFs_RcWY"
      },
      "outputs": [],
      "source": [
        "# To save time, you can just skip this step, as the output has already been stored\n",
        "\n",
        "X_train = read_images_to_array(train_dir)\n",
        "X_valid = read_images_to_array(valid_dir)\n",
        "X_test = read_images_to_array(test_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8mfBQ6-6wAr"
      },
      "outputs": [],
      "source": [
        "# save them to a file\n",
        "np.savez(\"/content/drive/MyDrive/images/rgb_dataset.npz\", traindata=X_train, validdata=X_valid, testdata=X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJO0MrW920F2"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXcsq7vj65Sd"
      },
      "outputs": [],
      "source": [
        "# To save time, you can just skip this step, as the output has already been stored\n",
        "\n",
        "with np.load(\"/content/drive/MyDrive/images/rgb_dataset.npz\") as data:\n",
        "    X_train = data[\"traindata\"]\n",
        "    X_valid = data[\"validdata\"]\n",
        "    X_test = data[\"testdata\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvyiFBTt4V2O",
        "outputId": "3e5a2de7-70be-4472-f737-792f224ceee9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape :  (63325, 28, 28, 3)\n",
            "X_valid shape :  (450, 28, 28, 3)\n",
            "X_test shape :  (450, 28, 28, 3)\n"
          ]
        }
      ],
      "source": [
        "print('X_train shape : ', X_train.shape)\n",
        "print('X_valid shape : ',X_valid.shape)\n",
        "print('X_test shape : ',X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q44OK9gMQx-V"
      },
      "source": [
        "### Mini-batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXf9eMkvQ2nK"
      },
      "outputs": [],
      "source": [
        "# generate random-shuffled mini-batches\n",
        "def random_mini_batches(image, label, mini_batch_size = 256):\n",
        "    dataset_size = image.shape[0] # number of training examples\n",
        "    mini_batches = []\n",
        "    # shuffle (image, label)\n",
        "    permutation = list(np.random.permutation(dataset_size))\n",
        "    shuffled_image = image[permutation, :, :, :]\n",
        "    shuffled_label = label[permutation]\n",
        "    # partition (shuffled_image, shuffled_label). Minus the end case.\n",
        "    complete_minibatches_number = math.floor(dataset_size / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, complete_minibatches_number):\n",
        "        mini_batch_image = shuffled_image[k * mini_batch_size: k * mini_batch_size + mini_batch_size, :, :, :]\n",
        "        mini_batch_label = shuffled_label[k * mini_batch_size: k * mini_batch_size + mini_batch_size]\n",
        "        mini_batch = (mini_batch_image, mini_batch_label)\n",
        "        mini_batches.append(mini_batch)\n",
        "    # handle the end case (last mini-batch < mini_batch_size)\n",
        "    if dataset_size % mini_batch_size != 0:\n",
        "        mini_batch_image = shuffled_image[complete_minibatches_number * mini_batch_size: dataset_size, :, :, :]\n",
        "        mini_batch_label = shuffled_label[complete_minibatches_number * mini_batch_size: dataset_size]\n",
        "        mini_batch = (mini_batch_image, mini_batch_label)\n",
        "        mini_batches.append(mini_batch)\n",
        "    return mini_batches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RViZpFnbQ25Z"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2CWDVBVRSAM"
      },
      "source": [
        "#### Zero pad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wBidWkaRcoK"
      },
      "outputs": [],
      "source": [
        "# padding for the matrix of images\n",
        "def zero_pad(X, pad):\n",
        "    X_pad = np.pad(X, ((0, ), (pad, ), (pad, ), (0, )), \"constant\", constant_values = (0, 0))\n",
        "    return X_pad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u1xzO_1RdDM"
      },
      "source": [
        "#### Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz8CwxVKRg9B"
      },
      "outputs": [],
      "source": [
        "# normalise the dataset\n",
        "def normalise(image):\n",
        "    image -= image.min()\n",
        "    image = image / image.max()\n",
        "    image = (image - np.mean(image)) / np.std(image)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP7aNAb-Rj9V"
      },
      "source": [
        "#### Load dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQzXhTqW4uqn"
      },
      "source": [
        "## Part 2 : CNN Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKPDZgh5VrkL"
      },
      "source": [
        "#### Initialisation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNVtxZdj4ua4"
      },
      "outputs": [],
      "source": [
        "# Initialisation of the weights & bias\n",
        "def initialise(kernel_shape, sigma = 0.01, bias_factor = 0.001):\n",
        "    bias_shape = (1, 1, 1, kernel_shape[-1]) if len(kernel_shape) == 4 else (kernel_shape[-1], )\n",
        "    weight = np.random.normal(0, sigma, kernel_shape)\n",
        "    bias = np.ones(bias_shape) * bias_factor\n",
        "    return weight, bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-uh1R6GVwq3"
      },
      "source": [
        "#### Softmax activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOTgArWBV0u1"
      },
      "outputs": [],
      "source": [
        "# Softmax activation function for the output layer\n",
        "def softmax(X):\n",
        "    X_softmax = np.exp(X) / np.array([np.sum(np.exp(X), axis = 1)]).T\n",
        "    return X_softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWIqQ3I9V5Et"
      },
      "source": [
        "#### Convolution Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBrmHH-HV6-M"
      },
      "outputs": [],
      "source": [
        "class Conv_Layer:\n",
        "    def __init__(self, kernel_shape, stride = 1, pad = 0, sigma = 0.01, bias_factor = 0.001):\n",
        "        self.weight, self.bias = initialise(kernel_shape, sigma, bias_factor)\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "    \n",
        "    def forward_propagation(self, input_map):\n",
        "        self.input_map = input_map\n",
        "        batch_size, height_input, width_input, _ = input_map.shape\n",
        "        f, _, _, channel_output = self.weight.shape\n",
        "        height_output = int((height_input + 2 * self.pad - f) / self.stride + 1)\n",
        "        width_output = int((width_input + 2 * self.pad - f) / self.stride + 1)\n",
        "        output_map = np.zeros((batch_size, height_output, width_output, channel_output))\n",
        "        input_map_pad = zero_pad(input_map, self.pad)\n",
        "        for height in range(height_output):\n",
        "            for width in range(width_output):\n",
        "                vertical_start, vertical_end = height * self.stride, height * self.stride + f\n",
        "                horizontal_start, horizontal_end = width * self.stride, width * self.stride + f\n",
        "                input_map_slice = input_map_pad[:, vertical_start: vertical_end, horizontal_start: horizontal_end, :]\n",
        "                output_map[:, height, width, :] = np.tensordot(input_map_slice, self.weight, axes = ([1, 2, 3], [0, 1, 2])) + self.bias\n",
        "        return output_map\n",
        "    \n",
        "    def back_propagation(self, d_output_map, learning_rate):\n",
        "        f, _, _, channel_output = self.weight.shape\n",
        "        _, height_output, width_output, channel_output = d_output_map.shape\n",
        "        d_input_map = np.zeros(self.input_map.shape)\n",
        "        d_weight = np.zeros(self.weight.shape)\n",
        "        d_bias = np.zeros((1, 1, 1, channel_output))\n",
        "        if self.pad != 0:\n",
        "            input_map_pad = zero_pad(self.input_map, self.pad)\n",
        "            d_input_map_pad = zero_pad(d_input_map, self.pad)\n",
        "        else:\n",
        "            input_map_pad = self.input_map\n",
        "            d_input_map_pad = d_input_map\n",
        "        for height in range(height_output):\n",
        "            for width in range(width_output):\n",
        "                vertical_start, vertical_end = height * self.stride, height * self.stride + f\n",
        "                horizontal_start, horizontal_end = width * self.stride, width * self.stride + f\n",
        "                input_map_slice = input_map_pad[:, vertical_start: vertical_end, horizontal_start: horizontal_end, :]\n",
        "                d_input_map_pad[:, vertical_start: vertical_end, horizontal_start: horizontal_end, :] += np.transpose(np.dot(self.weight, d_output_map[:, height, width, :].T), (3, 0, 1, 2))\n",
        "                d_weight += np.dot(np.transpose(input_map_slice, (1, 2, 3, 0)), d_output_map[:, height, width, :])\n",
        "                d_bias += np.sum(d_output_map[:, height, width, :], axis = 0)\n",
        "        d_input_map = d_input_map_pad if self.pad == 0 else d_input_map_pad[:, self.pad: -self.pad, self.pad: -self.pad, :]\n",
        "        self.weight -= learning_rate * d_weight\n",
        "        self.bias -= learning_rate * d_bias\n",
        "        self.input_map = None\n",
        "        return d_input_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT1pu8CHWATy"
      },
      "source": [
        "#### Sigmoid Activation Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGVpWYchWDxb"
      },
      "outputs": [],
      "source": [
        "class Sigmoid_Layer:\n",
        "    def forward_propagation(self, input_map):\n",
        "        self.output_map = 1 / (1 + np.exp(-input_map))\n",
        "        return self.output_map\n",
        "\n",
        "    \n",
        "    def back_propagation(self, d_output_map):\n",
        "        d_input_map = np.multiply(d_output_map, np.multiply(self.output_map, 1 - self.output_map))\n",
        "        self.output_map = None\n",
        "        return d_input_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoiFpvvyWFjb"
      },
      "source": [
        "#### Max-Pooling Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLoEYEJBWIEu"
      },
      "outputs": [],
      "source": [
        "class MaxPool_Layer:\n",
        "    def __init__(self, stride = 2, f = 2):\n",
        "        self.stride = stride\n",
        "        self.f = f\n",
        "\n",
        "    def forward_propagation(self, input_map):\n",
        "        self.input_map = input_map\n",
        "        batch_size, height_input, width_input, channel = input_map.shape\n",
        "        height_output = int(1 + (height_input - self.f) / self.stride)\n",
        "        width_output = int(1 + (width_input - self.f) / self.stride)\n",
        "        output_map = np.zeros((batch_size, height_output, width_output, channel))\n",
        "        for height in range(height_output):\n",
        "            for width in range(width_output):\n",
        "                vertical_start, vertical_end = height * self.stride, height * self.stride + self.f\n",
        "                horizontal_start, horizontal_end = width * self.stride, width * self.stride + self.f\n",
        "                input_map_slice = input_map[:, vertical_start: vertical_end, horizontal_start: horizontal_end, :]\n",
        "                output_map[:, height, width, :] = np.max(input_map_slice, axis = (1, 2))\n",
        "        return output_map\n",
        "\n",
        "    def back_propagation(self, d_output_map):\n",
        "        _, height_output, width_output, _ = d_output_map.shape\n",
        "        d_input_map = np.zeros(self.input_map.shape)\n",
        "        for height in range(height_output):\n",
        "            for width in range(width_output):\n",
        "                vertical_start, vertical_end = height * self.stride, height * self.stride + self.f\n",
        "                horizontal_start, horizontal_end = width * self.stride, width * self.stride + self.f\n",
        "                input_map_slice = self.input_map[:, vertical_start: vertical_end, horizontal_start: horizontal_end, :]\n",
        "                input_map_slice = np.transpose(input_map_slice, (1, 2, 3, 0))\n",
        "                mask = input_map_slice == input_map_slice.max((0, 1))\n",
        "                mask = np.transpose(mask, (3, 2, 0, 1))\n",
        "                d_input_map[:, vertical_start: vertical_end, horizontal_start: horizontal_end, :] += np.transpose(np.multiply(d_output_map[:, height, width, :][:, :, np.newaxis, np.newaxis], mask), (0, 2, 3, 1))\n",
        "        self.input_map = None\n",
        "        return d_input_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJSdtNvKWMzS"
      },
      "source": [
        "#### Fully Connected Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAk8gNm8WO0E"
      },
      "outputs": [],
      "source": [
        "class FC_Layer:\n",
        "    def __init__(self, weight_shape, sigma = 0.1, bias_factor = 0.01):\n",
        "        self.weight, self.bias = initialise(weight_shape, sigma, bias_factor)\n",
        "\n",
        "    def forward_propagation(self, input_array):\n",
        "        self.input_array = input_array\n",
        "        return np.matmul(input_array, self.weight) + self.bias\n",
        "\n",
        "    def back_propagation(self, d_output_array, learning_rate):\n",
        "        d_input_array = np.matmul(d_output_array, self.weight.T)\n",
        "        d_weight = np.matmul(self.input_array.T, d_output_array)\n",
        "        d_bias = np.sum(d_output_array.T, axis = 1)\n",
        "        self.weight -= learning_rate * d_weight\n",
        "        self.bias -= learning_rate * d_bias\n",
        "        self.input_array = None\n",
        "        return d_input_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZmZ6RoyWRRE"
      },
      "source": [
        "#### Fully Connected Output Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPV95GfcWTIC"
      },
      "outputs": [],
      "source": [
        "class FC_Output_Layer:\n",
        "    def __init__(self, weight_shape, sigma = 0.1, bias_factor = 0.01):\n",
        "        self.weight, self.bias = initialise(weight_shape, sigma, bias_factor)\n",
        "    \n",
        "    def forward_propagation(self, input_array, labels, mode):\n",
        "        self.input_array = input_array\n",
        "        self.labels = labels\n",
        "        self.output_array = np.matmul(input_array, self.weight) + self.bias\n",
        "        output = softmax(self.output_array)\n",
        "        predictions = np.argmax(output, axis = 1)\n",
        "        if mode == \"train\":\n",
        "            cost_value = -np.log(output[range(output.shape[0]), labels])\n",
        "            return np.sum(cost_value)\n",
        "        elif mode == \"test\":\n",
        "            acc = np.sum(labels == predictions)\n",
        "            return acc, predictions\n",
        "    \n",
        "    def back_propagation(self, learning_rate):\n",
        "        d_output_array = softmax(self.output_array)\n",
        "        d_output_array[range(d_output_array.shape[0]), self.labels] -= 1\n",
        "        d_output_array = d_output_array / d_output_array.shape[0]\n",
        "        d_input_array = np.matmul(d_output_array, self.weight.T)\n",
        "        d_weight = np.matmul(self.input_array.T, d_output_array)\n",
        "        d_bias = np.sum(d_output_array.T, axis = 1)\n",
        "        self.weight -= learning_rate * d_weight\n",
        "        self.bias -= learning_rate * d_bias\n",
        "        self.input_array, self.labels, self.output_array = None, None, None\n",
        "        return d_input_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqQ6D8caRmaX"
      },
      "outputs": [],
      "source": [
        "def load_dataset(X_dataset, label):\n",
        "    # data preprocessing\n",
        "    image_normalised_pad = normalise(zero_pad(X_dataset, 2))\n",
        "    return (image_normalised_pad, label)"
      ]
    }
  ]
}