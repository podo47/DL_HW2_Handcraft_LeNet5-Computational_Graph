{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMo/e1D8x59D/92Tl7EkQ96",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/podo47/DL_HW2_Handcraft_LeNet5-Computational_Graph/blob/main/DL_HW2_Handcraft_Lenet5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HK2yVHbvCxQ8"
      },
      "source": [
        "# LeNet-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1y4w23pCuO4"
      },
      "source": [
        "## Mount to drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFVWU4lUzzx9",
        "outputId": "71078879-61e5-4b4a-99dc-0a74270db433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkTGS_ltC66b"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOZA2Q0-C8U6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt \n",
        "from PIL import Image\n",
        "import cv2\n",
        "from multiprocessing.pool import ThreadPool\n",
        "import time\n",
        "import pickle\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzmxL4THDAU0"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWymwZ1EDC3O"
      },
      "source": [
        "## Part 1 : Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DloSn-46LTli"
      },
      "source": [
        "### Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sQGlA59MDECa"
      },
      "outputs": [],
      "source": [
        "train = pd.read_csv('/content/drive/MyDrive/images/train.txt', sep=\" \",header=None)\n",
        "train_dir = np.array(train[0])\n",
        "train_y = np.array(train[1])\n",
        "\n",
        "valid = pd.read_csv('/content/drive/MyDrive/images/val.txt', sep=\" \",header=None)\n",
        "valid_dir = np.array(valid[0])\n",
        "valid_y = np.array(valid[1])\n",
        "\n",
        "test = pd.read_csv('/content/drive/MyDrive/images/test.txt', sep=\" \",header=None)\n",
        "test_dir = np.array(test[0])\n",
        "test_y = np.array(test[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6BAhqR42q1U"
      },
      "source": [
        "### Read images to array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqlepSy2_-K9"
      },
      "outputs": [],
      "source": [
        "def read_image(imgname):\n",
        "    img = cv2.imread('/content/drive/MyDrive/'+imgname)\n",
        "    img = cv2.resize(img, (28, 28))\n",
        "    return img\n",
        "\n",
        "def read_images_to_array(data_dir):\n",
        "    pool = ThreadPool(processes=2) # 指定使用 2 個進程\n",
        "    X = pool.map(read_image, data_dir)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    X = np.array(X)\n",
        "    return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_1nH6Gm2wcZ"
      },
      "source": [
        "#### Save data (can skip)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDctbFs_RcWY"
      },
      "outputs": [],
      "source": [
        "# To save time, you can just skip this step, as the output has already been stored\n",
        "\n",
        "X_train = read_images_to_array(train_dir)\n",
        "X_valid = read_images_to_array(valid_dir)\n",
        "X_test = read_images_to_array(test_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8mfBQ6-6wAr"
      },
      "outputs": [],
      "source": [
        "# save them to a file\n",
        "np.savez(\"/content/drive/MyDrive/images/rgb_dataset.npz\", traindata=X_train, validdata=X_valid, testdata=X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJO0MrW920F2"
      },
      "source": [
        "#### Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXcsq7vj65Sd"
      },
      "outputs": [],
      "source": [
        "# To save time, you can just skip this step, as the output has already been stored\n",
        "\n",
        "with np.load(\"/content/drive/MyDrive/images/rgb_dataset.npz\") as data:\n",
        "    X_train = data[\"traindata\"]\n",
        "    X_valid = data[\"validdata\"]\n",
        "    X_test = data[\"testdata\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvyiFBTt4V2O",
        "outputId": "3e5a2de7-70be-4472-f737-792f224ceee9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "X_train shape :  (63325, 28, 28, 3)\n",
            "X_valid shape :  (450, 28, 28, 3)\n",
            "X_test shape :  (450, 28, 28, 3)\n"
          ]
        }
      ],
      "source": [
        "print('X_train shape : ', X_train.shape)\n",
        "print('X_valid shape : ',X_valid.shape)\n",
        "print('X_test shape : ',X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q44OK9gMQx-V"
      },
      "source": [
        "### Mini-batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXf9eMkvQ2nK"
      },
      "outputs": [],
      "source": [
        "# generate random-shuffled mini-batches\n",
        "def random_mini_batches(image, label, mini_batch_size = 256):\n",
        "    dataset_size = image.shape[0] # number of training examples\n",
        "    mini_batches = []\n",
        "    # shuffle (image, label)\n",
        "    permutation = list(np.random.permutation(dataset_size))\n",
        "    shuffled_image = image[permutation, :, :, :]\n",
        "    shuffled_label = label[permutation]\n",
        "    # partition (shuffled_image, shuffled_label). Minus the end case.\n",
        "    complete_minibatches_number = math.floor(dataset_size / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
        "    for k in range(0, complete_minibatches_number):\n",
        "        mini_batch_image = shuffled_image[k * mini_batch_size: k * mini_batch_size + mini_batch_size, :, :, :]\n",
        "        mini_batch_label = shuffled_label[k * mini_batch_size: k * mini_batch_size + mini_batch_size]\n",
        "        mini_batch = (mini_batch_image, mini_batch_label)\n",
        "        mini_batches.append(mini_batch)\n",
        "    # handle the end case (last mini-batch < mini_batch_size)\n",
        "    if dataset_size % mini_batch_size != 0:\n",
        "        mini_batch_image = shuffled_image[complete_minibatches_number * mini_batch_size: dataset_size, :, :, :]\n",
        "        mini_batch_label = shuffled_label[complete_minibatches_number * mini_batch_size: dataset_size]\n",
        "        mini_batch = (mini_batch_image, mini_batch_label)\n",
        "        mini_batches.append(mini_batch)\n",
        "    return mini_batches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RViZpFnbQ25Z"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2CWDVBVRSAM"
      },
      "source": [
        "#### Zero pad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wBidWkaRcoK"
      },
      "outputs": [],
      "source": [
        "# padding for the matrix of images\n",
        "def zero_pad(X, pad):\n",
        "    X_pad = np.pad(X, ((0, ), (pad, ), (pad, ), (0, )), \"constant\", constant_values = (0, 0))\n",
        "    return X_pad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1u1xzO_1RdDM"
      },
      "source": [
        "#### Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xz8CwxVKRg9B"
      },
      "outputs": [],
      "source": [
        "# normalise the dataset\n",
        "def normalise(image):\n",
        "    image -= image.min()\n",
        "    image = image / image.max()\n",
        "    image = (image - np.mean(image)) / np.std(image)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DP7aNAb-Rj9V"
      },
      "source": [
        "#### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(X_dataset, label):\n",
        "    # data preprocessing\n",
        "    image_normalised_pad = normalise(zero_pad(X_dataset, 2))\n",
        "    return (image_normalised_pad, label)"
      ],
      "metadata": {
        "id": "UMVe3dW90fTG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQzXhTqW4uqn"
      },
      "source": [
        "## Part 2 : CNN Layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKPDZgh5VrkL"
      },
      "source": [
        "#### Initialisation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNVtxZdj4ua4"
      },
      "outputs": [],
      "source": [
        "# Initialisation of the weights & bias\n",
        "def initialise(kernel_shape, sigma = 0.01, bias_factor = 0.001):\n",
        "    bias_shape = (1, 1, 1, kernel_shape[-1]) if len(kernel_shape) == 4 else (kernel_shape[-1], )\n",
        "    weight = np.random.normal(0, sigma, kernel_shape)\n",
        "    bias = np.ones(bias_shape) * bias_factor\n",
        "    return weight, bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-uh1R6GVwq3"
      },
      "source": [
        "#### Softmax activation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOTgArWBV0u1"
      },
      "outputs": [],
      "source": [
        "# Softmax activation function for the output layer\n",
        "def softmax(X):\n",
        "    X_softmax = np.exp(X) / np.array([np.sum(np.exp(X), axis = 1)]).T\n",
        "    return X_softmax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWIqQ3I9V5Et"
      },
      "source": [
        "#### Convolution Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FBrmHH-HV6-M"
      },
      "outputs": [],
      "source": [
        "class Conv_Layer:\n",
        "    def __init__(self, kernel_shape, stride = 1, pad = 0, sigma = 0.01, bias_factor = 0.001):\n",
        "        self.weight, self.bias = initialise(kernel_shape, sigma, bias_factor)\n",
        "        self.stride = stride\n",
        "        self.pad = pad\n",
        "    \n",
        "    def forward_propagation(self, input_map):\n",
        "        self.input_map = input_map\n",
        "        batch_size, height_input, width_input, _ = input_map.shape\n",
        "        f, _, _, channel_output = self.weight.shape\n",
        "        height_output = int((height_input + 2 * self.pad - f) / self.stride + 1)\n",
        "        width_output = int((width_input + 2 * self.pad - f) / self.stride + 1)\n",
        "        output_map = np.zeros((batch_size, height_output, width_output, channel_output))\n",
        "        input_map_pad = zero_pad(input_map, self.pad)\n",
        "        for height in range(height_output):\n",
        "            for width in range(width_output):\n",
        "                vertical_start, vertical_end = height * self.stride, height * self.stride + f\n",
        "                horizontal_start, horizontal_end = width * self.stride, width * self.stride + f\n",
        "                input_map_slice = input_map_pad[:, vertical_start: vertical_end, horizontal_start: horizontal_end, :]\n",
        "                output_map[:, height, width, :] = np.tensordot(input_map_slice, self.weight, axes = ([1, 2, 3], [0, 1, 2])) + self.bias\n",
        "        return output_map\n",
        "    \n",
        "    def back_propagation(self, d_output_map, learning_rate):\n",
        "        f, _, _, channel_output = self.weight.shape\n",
        "        _, height_output, width_output, channel_output = d_output_map.shape\n",
        "        d_input_map = np.zeros(self.input_map.shape)\n",
        "        d_weight = np.zeros(self.weight.shape)\n",
        "        d_bias = np.zeros((1, 1, 1, channel_output))\n",
        "        if self.pad != 0:\n",
        "            input_map_pad = zero_pad(self.input_map, self.pad)\n",
        "            d_input_map_pad = zero_pad(d_input_map, self.pad)\n",
        "        else:\n",
        "            input_map_pad = self.input_map\n",
        "            d_input_map_pad = d_input_map\n",
        "        for height in range(height_output):\n",
        "            for width in range(width_output):\n",
        "                vertical_start, vertical_end = height * self.stride, height * self.stride + f\n",
        "                horizontal_start, horizontal_end = width * self.stride, width * self.stride + f\n",
        "                input_map_slice = input_map_pad[:, vertical_start: vertical_end, horizontal_start: horizontal_end, :]\n",
        "                d_input_map_pad[:, vertical_start: vertical_end, horizontal_start: horizontal_end, :] += np.transpose(np.dot(self.weight, d_output_map[:, height, width, :].T), (3, 0, 1, 2))\n",
        "                d_weight += np.dot(np.transpose(input_map_slice, (1, 2, 3, 0)), d_output_map[:, height, width, :])\n",
        "                d_bias += np.sum(d_output_map[:, height, width, :], axis = 0)\n",
        "        d_input_map = d_input_map_pad if self.pad == 0 else d_input_map_pad[:, self.pad: -self.pad, self.pad: -self.pad, :]\n",
        "        self.weight -= learning_rate * d_weight\n",
        "        self.bias -= learning_rate * d_bias\n",
        "        self.input_map = None\n",
        "        return d_input_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uT1pu8CHWATy"
      },
      "source": [
        "#### Sigmoid Activation Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGVpWYchWDxb"
      },
      "outputs": [],
      "source": [
        "class Sigmoid_Layer:\n",
        "    def forward_propagation(self, input_map):\n",
        "        self.output_map = 1 / (1 + np.exp(-input_map))\n",
        "        return self.output_map\n",
        "\n",
        "    \n",
        "    def back_propagation(self, d_output_map):\n",
        "        d_input_map = np.multiply(d_output_map, np.multiply(self.output_map, 1 - self.output_map))\n",
        "        self.output_map = None\n",
        "        return d_input_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoiFpvvyWFjb"
      },
      "source": [
        "#### Max-Pooling Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pLoEYEJBWIEu"
      },
      "outputs": [],
      "source": [
        "class MaxPool_Layer:\n",
        "    def __init__(self, stride = 2, f = 2):\n",
        "        self.stride = stride\n",
        "        self.f = f\n",
        "\n",
        "    def forward_propagation(self, input_map):\n",
        "        self.input_map = input_map\n",
        "        batch_size, height_input, width_input, channel = input_map.shape\n",
        "        height_output = int(1 + (height_input - self.f) / self.stride)\n",
        "        width_output = int(1 + (width_input - self.f) / self.stride)\n",
        "        output_map = np.zeros((batch_size, height_output, width_output, channel))\n",
        "        for height in range(height_output):\n",
        "            for width in range(width_output):\n",
        "                vertical_start, vertical_end = height * self.stride, height * self.stride + self.f\n",
        "                horizontal_start, horizontal_end = width * self.stride, width * self.stride + self.f\n",
        "                input_map_slice = input_map[:, vertical_start: vertical_end, horizontal_start: horizontal_end, :]\n",
        "                output_map[:, height, width, :] = np.max(input_map_slice, axis = (1, 2))\n",
        "        return output_map\n",
        "\n",
        "    def back_propagation(self, d_output_map):\n",
        "        _, height_output, width_output, _ = d_output_map.shape\n",
        "        d_input_map = np.zeros(self.input_map.shape)\n",
        "        for height in range(height_output):\n",
        "            for width in range(width_output):\n",
        "                vertical_start, vertical_end = height * self.stride, height * self.stride + self.f\n",
        "                horizontal_start, horizontal_end = width * self.stride, width * self.stride + self.f\n",
        "                input_map_slice = self.input_map[:, vertical_start: vertical_end, horizontal_start: horizontal_end, :]\n",
        "                input_map_slice = np.transpose(input_map_slice, (1, 2, 3, 0))\n",
        "                mask = input_map_slice == input_map_slice.max((0, 1))\n",
        "                mask = np.transpose(mask, (3, 2, 0, 1))\n",
        "                d_input_map[:, vertical_start: vertical_end, horizontal_start: horizontal_end, :] += np.transpose(np.multiply(d_output_map[:, height, width, :][:, :, np.newaxis, np.newaxis], mask), (0, 2, 3, 1))\n",
        "        self.input_map = None\n",
        "        return d_input_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJSdtNvKWMzS"
      },
      "source": [
        "#### Fully Connected Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAk8gNm8WO0E"
      },
      "outputs": [],
      "source": [
        "class FC_Layer:\n",
        "    def __init__(self, weight_shape, sigma = 0.1, bias_factor = 0.01):\n",
        "        self.weight, self.bias = initialise(weight_shape, sigma, bias_factor)\n",
        "\n",
        "    def forward_propagation(self, input_array):\n",
        "        self.input_array = input_array\n",
        "        return np.matmul(input_array, self.weight) + self.bias\n",
        "\n",
        "    def back_propagation(self, d_output_array, learning_rate):\n",
        "        d_input_array = np.matmul(d_output_array, self.weight.T)\n",
        "        d_weight = np.matmul(self.input_array.T, d_output_array)\n",
        "        d_bias = np.sum(d_output_array.T, axis = 1)\n",
        "        self.weight -= learning_rate * d_weight\n",
        "        self.bias -= learning_rate * d_bias\n",
        "        self.input_array = None\n",
        "        return d_input_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZmZ6RoyWRRE"
      },
      "source": [
        "#### Fully Connected Output Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPV95GfcWTIC"
      },
      "outputs": [],
      "source": [
        "class FC_Output_Layer:\n",
        "    def __init__(self, weight_shape, sigma = 0.1, bias_factor = 0.01):\n",
        "        self.weight, self.bias = initialise(weight_shape, sigma, bias_factor)\n",
        "    \n",
        "    def forward_propagation(self, input_array, labels, mode):\n",
        "        self.input_array = input_array\n",
        "        self.labels = labels\n",
        "        self.output_array = np.matmul(input_array, self.weight) + self.bias\n",
        "        output = softmax(self.output_array)\n",
        "        predictions = np.argmax(output, axis = 1)\n",
        "        if mode == \"train\":\n",
        "            cost_value = -np.log(output[range(output.shape[0]), labels])\n",
        "            return np.sum(cost_value)\n",
        "        elif mode == \"test\":\n",
        "            acc = np.sum(labels == predictions)\n",
        "            return acc, predictions\n",
        "    \n",
        "    def back_propagation(self, learning_rate):\n",
        "        d_output_array = softmax(self.output_array)\n",
        "        d_output_array[range(d_output_array.shape[0]), self.labels] -= 1\n",
        "        d_output_array = d_output_array / d_output_array.shape[0]\n",
        "        d_input_array = np.matmul(d_output_array, self.weight.T)\n",
        "        d_weight = np.matmul(self.input_array.T, d_output_array)\n",
        "        d_bias = np.sum(d_output_array.T, axis = 1)\n",
        "        self.weight -= learning_rate * d_weight\n",
        "        self.bias -= learning_rate * d_bias\n",
        "        self.input_array, self.labels, self.output_array = None, None, None\n",
        "        return d_input_array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1SaDTnR0-koQ"
      },
      "source": [
        "## Part 3 : LeNet-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "evmgFvmFR9P-"
      },
      "outputs": [],
      "source": [
        "class LeNet5:\n",
        "    def __init__(self):\n",
        "\n",
        "        kernel_shape = {\"C1\": (5, 5, 3, 6),\n",
        "                        \"C3\": (5, 5, 6, 16),\n",
        "                        \"C5\": (5, 5, 16, 120),\n",
        "                        \"F6\": (120, 84),\n",
        "                        \"F7\": (84, 50)}\n",
        "\n",
        "        self.C1 = Conv_Layer(kernel_shape[\"C1\"], sigma = 0.1, bias_factor = 0.01)\n",
        "        self.Sigmoid1 = Sigmoid_Layer()\n",
        "        self.S2 = MaxPool_Layer()\n",
        "        self.C3 = Conv_Layer(kernel_shape[\"C3\"], sigma = 0.1, bias_factor = 0.01)\n",
        "        self.Sigmoid2 = Sigmoid_Layer()\n",
        "        self.S4 = MaxPool_Layer()\n",
        "        self.C5 = Conv_Layer(kernel_shape[\"C5\"], sigma = 0.1, bias_factor = 0.01)\n",
        "        self.Sigmoid3 = Sigmoid_Layer()\n",
        "        self.F6 = FC_Layer(kernel_shape[\"F6\"], sigma = 0.1, bias_factor = 0.01)\n",
        "        self.Sigmoid4 = Sigmoid_Layer()\n",
        "        self.F7 = FC_Output_Layer(kernel_shape[\"F7\"], sigma = 0.1, bias_factor = 0.01)\n",
        "\n",
        "    def forward_propagation(self, input_image, input_label, mode):\n",
        "        C1_FP = self.C1.forward_propagation(input_image)\n",
        "        Sigmoid1_FP = self.Sigmoid1.forward_propagation(C1_FP)\n",
        "        S2_FP = self.S2.forward_propagation(Sigmoid1_FP)\n",
        "        C3_FP = self.C3.forward_propagation(S2_FP)\n",
        "        Sigmoid2_FP = self.Sigmoid2.forward_propagation(C3_FP)\n",
        "        S4_FP = self.S4.forward_propagation(Sigmoid2_FP)\n",
        "        C5_FP = self.C5.forward_propagation(S4_FP)\n",
        "        Sigmoid3_FP = self.Sigmoid3.forward_propagation(C5_FP)\n",
        "        Sigmoid3_FP = Sigmoid3_FP[:, 0, 0, :]\n",
        "        F6_FP = self.F6.forward_propagation(Sigmoid3_FP)\n",
        "        Sigmoid4_FP = self.Sigmoid4.forward_propagation(F6_FP)\n",
        "        return self.F7.forward_propagation(Sigmoid4_FP, input_label, mode)\n",
        "\n",
        "    def back_propagation(self, learning_rate):\n",
        "        F7_BP = self.F7.back_propagation(learning_rate)\n",
        "        Sigmoid4_BP = self.Sigmoid4.back_propagation(F7_BP)\n",
        "        F6_BP = self.F6.back_propagation(Sigmoid4_BP, learning_rate)\n",
        "        F6_BP = F6_BP[:, np.newaxis, np.newaxis, :]\n",
        "        Sigmoid3_BP = self.Sigmoid3.back_propagation(F6_BP)\n",
        "        C5_BP = self.C5.back_propagation(Sigmoid3_BP, learning_rate)\n",
        "        S4_BP = self.S4.back_propagation(C5_BP)\n",
        "        Sigmoid2_BP = self.Sigmoid2.back_propagation(S4_BP)\n",
        "        C3_BP = self.C3.back_propagation(Sigmoid2_BP, learning_rate)\n",
        "        S2_BP = self.S2.back_propagation(C3_BP)\n",
        "        Sigmoid1_BP = self.Sigmoid1.back_propagation(S2_BP)\n",
        "        self.C1.back_propagation(Sigmoid1_BP, learning_rate)\n",
        "    \n",
        "    def extract_model(self):\n",
        "        temp_model = LeNet5()\n",
        "        temp_model.C1.weight = self.C1.weight\n",
        "        temp_model.C1.bias = self.C1.bias\n",
        "        temp_model.C1.stride = self.C1.stride\n",
        "        temp_model.C1.pad = self.C1.pad\n",
        "        temp_model.S2.stride = self.S2.stride\n",
        "        temp_model.S2.f = self.S2.f\n",
        "        temp_model.C3.weight = self.C3.weight\n",
        "        temp_model.C3.bias = self.C3.bias\n",
        "        temp_model.C3.stride = self.C3.stride\n",
        "        temp_model.C3.pad = self.C3.pad\n",
        "        temp_model.S4.stride = self.S4.stride\n",
        "        temp_model.S4.f = self.S4.f\n",
        "        temp_model.C5.weight = self.C5.weight\n",
        "        temp_model.C5.bias = self.C5.bias\n",
        "        temp_model.C5.stride = self.C5.stride\n",
        "        temp_model.C5.pad = self.C5.pad\n",
        "        temp_model.F6.weight = self.F6.weight\n",
        "        temp_model.F6.bias = self.F6.bias\n",
        "        temp_model.F7.weight = self.F7.weight\n",
        "        temp_model.F7.bias = self.F7.bias\n",
        "        return temp_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuvVzB3BR6bK"
      },
      "source": [
        "## Part 4 : Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fxe24Bv9SArB"
      },
      "outputs": [],
      "source": [
        "def train(model, train_data, valid_data, epoches, learning_rate_list, batch_size):\n",
        "    # training loops\n",
        "    start_time = time.time()\n",
        "    acc_rate_list = []\n",
        "\n",
        "    for epoch in range(0, epoches):\n",
        "        print(\"---------- epoch\", epoch + 1, \"begin ----------\")\n",
        "        learning_rate = learning_rate_list[epoch]\n",
        "        # print information\n",
        "        print(\"learning rate: {}\".format(learning_rate))\n",
        "        print(\"batch size: {}\".format(batch_size))\n",
        "        # loop over each batch\n",
        "        start_time_epoch = time.time()\n",
        "        cost = 0\n",
        "        mini_batches = random_mini_batches(train_data[0], train_data[1], batch_size)\n",
        "        print(\"Training:\")\n",
        "        for i in tqdm(range(len(mini_batches))):\n",
        "            batch_image, batch_label = mini_batches[i]\n",
        "            loss = model.forward_propagation(batch_image, batch_label, 'train')\n",
        "            cost += loss\n",
        "            model.back_propagation(learning_rate)\n",
        "        print(\"Done, total cost of epoch {}: {}\".format(epoch + 1, cost))\n",
        "        \n",
        "        acc_train, _ = model.forward_propagation(train_data[0], train_data[1], 'test')\n",
        "        acc_valid, _ = model.forward_propagation(valid_data[0], valid_data[1], 'test')\n",
        "\n",
        "\n",
        "        acc_rate_list.append([acc_train / len(train_data[1]), acc_valid / len(valid_data[1])])\n",
        "        print(\"0/1 Accuracy of training set:\", acc_train, \"/\", len(train_data[1]))\n",
        "        print(\"0/1 Accuracy of valid set:\", acc_valid, \"/\", len(valid_data[1]))\n",
        "        print(\"Time used:\", time.time() - start_time_epoch, \"sec\")\n",
        "        print(\"---------- epoch\", epoch + 1, \"end ------------\")\n",
        "        with open(\"/content/drive/MyDrive/DL/HW2/LeNet5_model/lenet5_data_\" + str(epoch + 1) + \".pkl\", \"wb\") as output:\n",
        "            pickle.dump(model.extract_model(), output, pickle.HIGHEST_PROTOCOL)\n",
        "    acc_rate_list = np.array(acc_rate_list).T\n",
        "    print(\"Total time used:\", time.time() - start_time, \"sec\")\n",
        "  \n",
        "    return acc_rate_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0x3gOu7hSCIU"
      },
      "source": [
        "## Part 5 : Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "JinTuinqSFmi"
      },
      "outputs": [],
      "source": [
        "def test(model_path, test_data):\n",
        "    # read model\n",
        "    with open(model_path, \"rb\") as model_file:\n",
        "        model = pickle.load(model_file)\n",
        "    print(\"Testing with {}:\".format(model_path))\n",
        "    acc, predictions = model.forward_propagation(test_data[0], test_data[1], \"test\")\n",
        "    print(\"Accuracy of test set:\", acc / len(predictions))\n",
        "\n",
        "    return acc / len(predictions), predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYxi60N7TBbK"
      },
      "source": [
        "## Part 6 : Train Model and Draw Performance\n",
        "To save time, you can just skip this step, as the pretrained has already been stored in file \"LeNet5_model\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QzmNlMfqA34k"
      },
      "outputs": [],
      "source": [
        "train_data = load_dataset(X_train, train_y)\n",
        "valid_data = load_dataset(X_valid, valid_y)\n",
        "test_data = load_dataset(X_test, test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cKrNHPzDBxrZ"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "epoches = 25\n",
        "learning_rate_list = np.array([1e-1] * 5 + [5e-2] * 20 )\n",
        "model = LeNet5()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36Wm2C7VE6Se"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "acc_list = train(model, train_data, valid_data, epoches, learning_rate_list, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edSFOz6aCDze"
      },
      "outputs": [],
      "source": [
        "x = np.arange(1, epoches + 1)\n",
        "plt.xlabel(\"epoches\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.plot(x, acc_list[0])\n",
        "plt.plot(x, acc_list[1])\n",
        "plt.grid(True)\n",
        "plt.legend([\"Training data\", \"Validaton data\"], loc = \"upper left\")\n",
        "plt.savefig(\"/content/drive/MyDrive/DL/HW2/LeNet5_model/LeNet5_accuracy.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3spYacNMniNi"
      },
      "source": [
        "**Save accuracy result (can skip)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ScFTLbzl_bX"
      },
      "outputs": [],
      "source": [
        "# save them to a file\n",
        "np.savez(\"/content/drive/MyDrive/DL/HW2/LeNet5_model/acc_list.npz\", train_acc=acc_list[0], valid_acc=acc_list[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmmtuGT_58wg"
      },
      "source": [
        "## Part 7 : Result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZOpJPGFpjbZ"
      },
      "source": [
        "**Load train and valid accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sE394NXjpiuZ"
      },
      "outputs": [],
      "source": [
        "with np.load(\"/content/drive/MyDrive/DL/HW2/LeNet5_model/acc_list.npz\") as data:\n",
        "    acc_train = data[\"train_acc\"]\n",
        "    acc_valid = data[\"valid_acc\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9F6WNIcf6B5A"
      },
      "outputs": [],
      "source": [
        "def answer(test_y, test_y_pred, data_name, output_name):\n",
        "  df = np.column_stack((test_y, test_y_pred))\n",
        "  test_result = pd.DataFrame(df, index=data_name, columns=['Answer','Prediction'])\n",
        "  test_result.to_csv('/content/drive/MyDrive/DL/HW2/LeNet5_model/' + output_name + '_test_result.csv')\n",
        "  return test_result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5E12cur7Csb"
      },
      "source": [
        "**Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ieB5TbEl7UwR",
        "outputId": "3da290df-6c89-4ea2-96f5-7ccc4234a533"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing with /content/drive/MyDrive/DL/HW2/LeNet5_model/lenet5_data_24.pkl:\n",
            "Accuracy of test set: 0.14444444444444443\n"
          ]
        }
      ],
      "source": [
        "LeNet5_test_acc, LeNet5_y_pred = test(\"/content/drive/MyDrive/DL/HW2/LeNet5_model/lenet5_data_\" + str(acc_valid.argmax() + 1) + \".pkl\", test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfqTiUpdlWvE",
        "outputId": "c6075e98-438d-47e3-d21e-f4147d53e9bf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.1444"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test accuracy\n",
        "LeNet5_test_acc = round(LeNet5_test_acc,4)\n",
        "LeNet5_test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "KluHYI2-7vtP",
        "outputId": "ed89ffdf-923f-41a4-941c-99064f4cb71e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-860273ca-2356-4ffe-9f48-d0a13e3e17b7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Answer</th>\n",
              "      <th>Prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>images/n02111277/n02111277_9420.JPEG</th>\n",
              "      <td>0</td>\n",
              "      <td>48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>images/n02111277/n02111277_9422.JPEG</th>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>images/n02111277/n02111277_9484.JPEG</th>\n",
              "      <td>0</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>images/n02111277/n02111277_951.JPEG</th>\n",
              "      <td>0</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>images/n02111277/n02111277_9518.JPEG</th>\n",
              "      <td>0</td>\n",
              "      <td>46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>images/n02172182/n02172182_974.JPEG</th>\n",
              "      <td>49</td>\n",
              "      <td>44</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>images/n02172182/n02172182_9765.JPEG</th>\n",
              "      <td>49</td>\n",
              "      <td>34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>images/n02172182/n02172182_9789.JPEG</th>\n",
              "      <td>49</td>\n",
              "      <td>49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>images/n02172182/n02172182_98.JPEG</th>\n",
              "      <td>49</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>images/n02172182/n02172182_981.JPEG</th>\n",
              "      <td>49</td>\n",
              "      <td>41</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>450 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-860273ca-2356-4ffe-9f48-d0a13e3e17b7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-860273ca-2356-4ffe-9f48-d0a13e3e17b7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-860273ca-2356-4ffe-9f48-d0a13e3e17b7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                      Answer  Prediction\n",
              "images/n02111277/n02111277_9420.JPEG       0          48\n",
              "images/n02111277/n02111277_9422.JPEG       0          32\n",
              "images/n02111277/n02111277_9484.JPEG       0          41\n",
              "images/n02111277/n02111277_951.JPEG        0          41\n",
              "images/n02111277/n02111277_9518.JPEG       0          46\n",
              "...                                      ...         ...\n",
              "images/n02172182/n02172182_974.JPEG       49          44\n",
              "images/n02172182/n02172182_9765.JPEG      49          34\n",
              "images/n02172182/n02172182_9789.JPEG      49          49\n",
              "images/n02172182/n02172182_98.JPEG        49          41\n",
              "images/n02172182/n02172182_981.JPEG       49          41\n",
              "\n",
              "[450 rows x 2 columns]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output_name = 'LeNet5'\n",
        "LeNet5_answer = answer(test_y, LeNet5_y_pred, test_dir, output_name)\n",
        "LeNet5_answer"
      ]
    }
  ]
}